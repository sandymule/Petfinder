{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "# Creating the Neural Network Using Base and Top Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "\n",
    "# path to the model weights file.\n",
    "weights_path = './VGG/vgg16_weights.h5'\n",
    "top_model_weights_path = './VGG/bottleneck_fc_model.h5'\n",
    "# dimensions of our images.\n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "train_data_dir = './Pictures/Train'\n",
    "validation_data_dir = './Pictures/Validation'\n",
    "nb_train_samples = 13675\n",
    "nb_validation_samples = 3435\n",
    "nb_epoch = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#Locate the files that do not open properly\n",
    "\n",
    "# import PIL\n",
    "# from glob import glob\n",
    "\n",
    "# for filename in glob(validation_data_dir + \"/*/*.png\"):\n",
    "#     try:\n",
    "#         _ = PIL.Image.open(filename)\n",
    "#         _.load()\n",
    "#     except IOError:\n",
    "#         print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#Remove files that do not open properly\n",
    "\n",
    "# from glob import glob\n",
    "# import os\n",
    "\n",
    "# for filename in glob(train_data_dir + \"/*/.DS_Store\"):\n",
    "#     try:\n",
    "#         print str(filename)\n",
    "# #         os.remove(filename)\n",
    "#     except IOError:\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "# Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n"
     ]
    }
   ],
   "source": [
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# build the VGG16 network\n",
    "base_model = Sequential()\n",
    "base_model.add(ZeroPadding2D((1, 1), input_shape=(3, img_width, img_height)))\n",
    "\n",
    "base_model.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_1'))\n",
    "base_model.add(ZeroPadding2D((1, 1)))\n",
    "base_model.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_2'))\n",
    "base_model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "base_model.add(ZeroPadding2D((1, 1)))\n",
    "base_model.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_1'))\n",
    "base_model.add(ZeroPadding2D((1, 1)))\n",
    "base_model.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_2'))\n",
    "base_model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "base_model.add(ZeroPadding2D((1, 1)))\n",
    "base_model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_1'))\n",
    "base_model.add(ZeroPadding2D((1, 1)))\n",
    "base_model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_2'))\n",
    "base_model.add(ZeroPadding2D((1, 1)))\n",
    "base_model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_3'))\n",
    "base_model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "base_model.add(ZeroPadding2D((1, 1)))\n",
    "base_model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_1'))\n",
    "base_model.add(ZeroPadding2D((1, 1)))\n",
    "base_model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_2'))\n",
    "base_model.add(ZeroPadding2D((1, 1)))\n",
    "base_model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_3'))\n",
    "base_model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "base_model.add(ZeroPadding2D((1, 1)))\n",
    "base_model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_1'))\n",
    "base_model.add(ZeroPadding2D((1, 1)))\n",
    "base_model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_2'))\n",
    "base_model.add(ZeroPadding2D((1, 1)))\n",
    "base_model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_3'))\n",
    "base_model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "# load the weights of the VGG16 networks\n",
    "# (trained on ImageNet, won the ILSVRC competition in 2014)\n",
    "# note: when there is a complete match between your model definition\n",
    "# and your weight savefile, you can simply call model.load_weights(filename)\n",
    "assert os.path.exists(weights_path), 'Model weights not found (see \"weights_path\" variable in script).'\n",
    "f = h5py.File(weights_path)\n",
    "for k in range(f.attrs['nb_layers']):\n",
    "    if k >= len(base_model.layers):\n",
    "        # we don't look at the last (fully-connected) layers in the savefile\n",
    "        break\n",
    "    g = f['layer_{}'.format(k)]\n",
    "    weights = [g['param_{}'.format(p)] for p in range(g.attrs['nb_params'])]\n",
    "    base_model.layers[k].set_weights(weights)\n",
    "base_model.save('./VGG/base_model.h5')\n",
    "f.close()\n",
    "print('Model loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#generator for training data\n",
    "\n",
    "# generator = datagen.flow_from_directory(\n",
    "#         train_data_dir,\n",
    "#         target_size=(img_width, img_height),\n",
    "#         batch_size=32,\n",
    "#         class_mode=None,\n",
    "#         shuffle=False)\n",
    "# bottleneck_features_train = model.predict_generator(generator, nb_train_samples)\n",
    "# np.save(open('./VGG/bottleneck_features_train.npy', 'w'), bottleneck_features_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#generator for test data\n",
    "\n",
    "# generator = datagen.flow_from_directory(\n",
    "#          validation_data_dir,\n",
    "#          target_size=(img_width, img_height),\n",
    "#          batch_size=32,\n",
    "#          class_mode=None,\n",
    "#          shuffle=False)\n",
    "# bottleneck_features_validation = model.predict_generator(generator, nb_validation_samples)\n",
    "# np.save(open('./VGG/bottleneck_features_validation.npy', 'w'), bottleneck_features_validation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "# Top Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#loading in train and test labels\n",
    "\n",
    "import pickle\n",
    "with open('puppy_labels.pkl','rb') as f:\n",
    "    train_labels, test_labels = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "encoded_train_labels = LabelEncoder().fit_transform(train_labels)\n",
    "encoded_train_labels = to_categorical(encoded_train_labels,30)\n",
    "\n",
    "encoded_test_labels = LabelEncoder().fit_transform(test_labels)\n",
    "encoded_test_labels = to_categorical(encoded_test_labels,30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3435, 30)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_train_labels.shape\n",
    "encoded_test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13675 samples, validate on 3435 samples\n",
      "Epoch 1/10\n",
      "13675/13675 [==============================] - 3s - loss: 3.0587 - acc: 0.1361 - val_loss: 2.3547 - val_acc: 0.3144\n",
      "Epoch 2/10\n",
      "13675/13675 [==============================] - 3s - loss: 2.4908 - acc: 0.2550 - val_loss: 2.0508 - val_acc: 0.3869\n",
      "Epoch 3/10\n",
      "13675/13675 [==============================] - 3s - loss: 2.3107 - acc: 0.3038 - val_loss: 1.8855 - val_acc: 0.4638\n",
      "Epoch 4/10\n",
      "13675/13675 [==============================] - 3s - loss: 2.1669 - acc: 0.3440 - val_loss: 1.8222 - val_acc: 0.4780\n",
      "Epoch 5/10\n",
      "13675/13675 [==============================] - 3s - loss: 2.0631 - acc: 0.3755 - val_loss: 1.7097 - val_acc: 0.5031\n",
      "Epoch 6/10\n",
      "13675/13675 [==============================] - 3s - loss: 1.9831 - acc: 0.3994 - val_loss: 1.8449 - val_acc: 0.4524\n",
      "Epoch 7/10\n",
      "13675/13675 [==============================] - 3s - loss: 1.9243 - acc: 0.4192 - val_loss: 1.7282 - val_acc: 0.4984\n",
      "Epoch 8/10\n",
      "13675/13675 [==============================] - 3s - loss: 1.8563 - acc: 0.4409 - val_loss: 1.6456 - val_acc: 0.5263\n",
      "Epoch 9/10\n",
      "13675/13675 [==============================] - 3s - loss: 1.7790 - acc: 0.4631 - val_loss: 1.6379 - val_acc: 0.5316\n",
      "Epoch 10/10\n",
      "13675/13675 [==============================] - 3s - loss: 1.7495 - acc: 0.4735 - val_loss: 1.6695 - val_acc: 0.5229\n"
     ]
    }
   ],
   "source": [
    "train_data = np.load(open('./VGG/bottleneck_features_train.npy'))\n",
    "train_labels = np.array(encoded_train_labels)\n",
    "\n",
    "validation_data = np.load(open('./VGG/bottleneck_features_validation.npy'))\n",
    "validation_labels = np.array(encoded_test_labels)\n",
    "\n",
    "top_model = Sequential()\n",
    "top_model.add(Flatten(input_shape=train_data.shape[1:]))\n",
    "top_model.add(Dense(256, activation='relu'))\n",
    "top_model.add(Dropout(0.5))\n",
    "top_model.add(Dense(30, activation='softmax'))\n",
    "\n",
    "top_model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "top_model.fit(train_data, train_labels,\n",
    "      nb_epoch=nb_epoch, batch_size=32,\n",
    "      validation_data=(validation_data, validation_labels))\n",
    "top_model.save_weights(top_model_weights_path)\n",
    "\n",
    "#make sure to save the model itself in addition to the weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/icarus/anaconda2/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "y_pred = top_model.predict(validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "y_test_confus = []\n",
    "y_pred_confus = []\n",
    "for elem in encoded_test_labels:\n",
    "    y_test_confus.append(list(elem).index(max(elem)))\n",
    "for item in y_pred:\n",
    "    y_pred_confus.append(list(item).index(max(item)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "confus_mat = confusion_matrix(y_test_confus, y_pred_confus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f4514746450>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWEAAAFdCAYAAADSR9wBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYUlqyBCWpIb+PzyRo55ifM7EAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f45147971d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.matshow(confus_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "# Obtaining Vectors for Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Subtensor{int64}.0, Elemwise{add,no_inplace}.0, Elemwise{add,no_inplace}.0, Subtensor{int64}.0)\n",
      "(Subtensor{int64}.0, Elemwise{add,no_inplace}.0, Elemwise{add,no_inplace}.0, Subtensor{int64}.0)\n",
      "(Subtensor{int64}.0, Elemwise{add,no_inplace}.0, Elemwise{add,no_inplace}.0, Subtensor{int64}.0)\n",
      "(Subtensor{int64}.0, Elemwise{add,no_inplace}.0, Elemwise{add,no_inplace}.0, Subtensor{int64}.0)\n",
      "(Subtensor{int64}.0, Elemwise{add,no_inplace}.0, Elemwise{add,no_inplace}.0, Subtensor{int64}.0)\n",
      "(Subtensor{int64}.0, Elemwise{add,no_inplace}.0, Elemwise{add,no_inplace}.0, Subtensor{int64}.0)\n",
      "(Subtensor{int64}.0, Elemwise{add,no_inplace}.0, Elemwise{add,no_inplace}.0, Subtensor{int64}.0)\n",
      "(Subtensor{int64}.0, Elemwise{add,no_inplace}.0, Elemwise{add,no_inplace}.0, Subtensor{int64}.0)\n",
      "(Subtensor{int64}.0, Elemwise{add,no_inplace}.0, Elemwise{add,no_inplace}.0, Subtensor{int64}.0)\n",
      "(Subtensor{int64}.0, Elemwise{add,no_inplace}.0, Elemwise{add,no_inplace}.0, Subtensor{int64}.0)\n",
      "(Subtensor{int64}.0, Elemwise{add,no_inplace}.0, Elemwise{add,no_inplace}.0, Subtensor{int64}.0)\n",
      "(Subtensor{int64}.0, Elemwise{add,no_inplace}.0, Elemwise{add,no_inplace}.0, Subtensor{int64}.0)\n",
      "(Subtensor{int64}.0, Elemwise{add,no_inplace}.0, Elemwise{add,no_inplace}.0, Subtensor{int64}.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Sam/anaconda2/lib/python2.7/site-packages/keras/models.py:136: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "base_model = load_model('./VGG/base_model.h5')\n",
    "top_model = load_model('./VGG/top_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "ename": "error",
     "evalue": "/tmp/opencv-20161020-7399-1yrk4nm/opencv-2.4.13.1/modules/imgproc/src/imgwarp.cpp:1968: error: (-215) ssize.area() > 0 in function resize\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-d6e6f7da6800>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Validation/Chihuahua/10.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m150\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m103.939\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m116.779\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m123.68\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: /tmp/opencv-20161020-7399-1yrk4nm/opencv-2.4.13.1/modules/imgproc/src/imgwarp.cpp:1968: error: (-215) ssize.area() > 0 in function resize\n"
     ]
    }
   ],
   "source": [
    "img = cv2.resize(cv2.imread('Validation/Chihuahua/10.png'), (150, 150)).astype(np.float32)\n",
    "\n",
    "img[:,:,0] -= 103.939\n",
    "img[:,:,1] -= 116.779\n",
    "img[:,:,2] -= 123.68\n",
    "img = img.transpose((2,0,1))\n",
    "img = np.expand_dims(img, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "bottleneck_features_test = base_model.predict(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "train_data = np.load(open('./VGG/bottleneck_features_train.npy'))\n",
    "test_model = Sequential()\n",
    "test_model.add(Flatten(input_shape=train_data.shape[1:]))\n",
    "test_model.add(Dense(256, activation='relu', weights=top_model.layers[1].get_weights()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "activation_vectors = test_model.predict(bottleneck_features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 256)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
